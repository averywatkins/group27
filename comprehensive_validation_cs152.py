# -*- coding: utf-8 -*-
"""comprehensive_validation_cs152.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uYkthEKGNpV-CFVzc2FWrFPFJPoKgK1D
"""

# !pip install transformers

# !pip install torch

import numpy as np
import pandas as pd
import csv
import random
import sys
import torch
from torch.utils.data import DataLoader, Dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from tqdm import tqdm
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

# Use BERT base uncased tokenizer and model
# Read more: https://huggingface.co/bert-base-uncased

tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

def load_model(model_path, device):
    model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)
    checkpoint = torch.load(model_path)
    model.load_state_dict(checkpoint['model'])
    model.to(device)

    return model

def load_csv_dataset(csv_file):
    csv.field_size_limit(sys.maxsize) # increase field size limit

    texts, labels = [], []
    with open(csv_file, 'r') as file:
        csv_reader = csv.reader(file)
        next(csv_reader)

        for row in csv_reader:
            try: # for first row only
                text = row[0] 
                label = int(row[1])

                if sys.getsizeof(text) > csv.field_size_limit(): continue # if field size exceeds limit, skip row

                texts.append(text)
                labels.append(int(label))

            except csv.Error: continue  # if error in parsing, skip row

    return texts, labels

# Function that evaluates model with data

def model_eval(testdev_loader, model, device):
    model.eval()

    val_predictions = []
    val_targets = []

    with torch.no_grad():
        pbar = tqdm(testdev_loader, desc="Validation", leave=True)
        for texts, labels in pbar:
            texts = list(texts)
            labels = list(labels)
            encoded_inputs = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')
            input_ids = encoded_inputs['input_ids'].to(device)
            attention_mask = encoded_inputs['attention_mask'].to(device)
            labels = torch.tensor(labels).to(device)

            outputs = model(input_ids, attention_mask=attention_mask)
            logits = outputs.logits
            predictions = torch.argmax(logits, dim=1)

            val_predictions.extend(predictions.cpu().numpy())
            val_targets.extend(labels.cpu().numpy())

    return val_predictions, val_targets

# Get accuracy, precision, recall, F1, and CM from validation. Save into output file.
def get_eval_metrics(val_predictions, val_targets, eval_output_file, labels_output_file):
    accuracy = accuracy_score(val_targets, val_predictions)
    precision = precision_score(val_targets, val_predictions)
    recall = recall_score(val_targets, val_predictions)
    f1 = f1_score(val_targets, val_predictions)
    cm = confusion_matrix(val_targets, val_predictions)

    print(f"Validation Accuracy: {accuracy}")
    print(f"Validation Precision: {precision}")
    print(f"Validation Recall: {recall}")
    print(f"Validation F1 Score: {f1}")
    print("Confusion Matrix:")
    print(cm)

    # save eval metrics, cm into output file
    metrics_data = np.array([accuracy, precision, recall, f1])
    metrics_data = np.vstack((metrics_data, cm.ravel()))
    np.savetxt(eval_output_file, metrics_data, delimiter=',', header="Accuracy, Precision, Recall, F1 Score")

    # also save targets and predictions for later access
    df = pd.DataFrame({'Targets': val_targets, 'Predictions': val_predictions})
    df.to_csv(labels_output_file, index=False)

# Generate test_loader for evaluation
test_csv = '/home/shreyaravi/pan12/pan12-test/test_labels.csv'
test_texts, test_labels = load_csv_dataset(test_csv)

"""
test_texts = [
    "I received a suspicious email today.",
    "The weather is beautiful outside.",
    "Always verify the authenticity of online sellers.",
    "The concert was amazing!",
    "Don't share your passwords with anyone."
]

test_labels = [1, 0, 1, 0, 1]
"""

# Shuffle test dataset randomly, create TextDataset
class TextDataset(Dataset):
    def __init__(self, texts, labels):
        self.texts = texts
        self.labels = labels

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, index):
        return self.texts[index], self.labels[index]


### Test
# Shuffle test
combined_data = list(zip(test_texts, test_labels))
random.shuffle(combined_data)
test_texts, test_labels = zip(*combined_data)

# Create test_loader
test_dataset = TextDataset(test_texts, test_labels)
test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)

# Generate preds for given data
model_paths = ["last_model.pt", "best_model.pt"]
eval_output_files = ["eval_last_model.csv", "eval_best_model.csv"]
labels_output_files = ["labels_last_model.csv", "labels_best_model.csv"]

for i, model_path in enumerate(model_paths):
  model = load_model(model_path, device)
  val_predictions, val_targets = model_eval(test_loader, model, device)
  get_eval_metrics(val_predictions, val_targets, eval_output_files[i], labels_output_files[i])
